{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b0950f7-2a5a-4c44-9aa0-9938214ede24",
   "metadata": {},
   "source": [
    "# Introduction to Retrieval Augmented Generation (RAG), In-context Learning, and Next-generation LLMOps\n",
    "\n",
    "### 1. Retrieval Augmented Generation (RAG):\n",
    "Retrieval Augmented Generation (RAG) is a novel approach that combines the strengths of large-scale retrieval systems with the generative capabilities of transformers like GPT. In a RAG-based system, when a question is posed, relevant documents or passages are retrieved from a corpus, and then fed alongside the query to a generative model. This two-step process enables the model to leverage both external knowledge from the corpus and its internal knowledge to produce more informed and contextually accurate responses.\n",
    "\n",
    "### 2. In-context Learning:\n",
    "Traditional machine learning models learn from extensive labeled datasets. In contrast, in-context learning pertains to models, especially language models, leveraging a few examples or context provided at inference time to tailor their outputs. This technique allows LLMs to be dynamically adapted to new tasks without undergoing explicit fine-tuning.\n",
    "\n",
    "### 3. LLM Chains/Pipelines:\n",
    "LLM chains or pipelines involve stringing together multiple stages or components of a system to achieve a complex task. For instance, in a RAG system, a retrieval component fetches relevant data from a database, followed by a generation component that constructs the final answer. By chaining different LLM modules, developers can build systems that harness diverse capabilities and can be modularly updated or optimized.\n",
    "\n",
    "### 4. RAG for On-Premise LLM Applications:\n",
    "With the growing need for data privacy and proprietary data handling, many enterprises seek solutions to harness the power of LLMs in-house. RAG provides a unique opportunity for such use-cases. By integrating RAG with on-premise data repositories, enterprises can build powerful LLM applications tailored to their specific needs, while ensuring data confidentiality.\n",
    "\n",
    "### 5. RAG and Fine-Tuning:\n",
    "While RAG is a powerful approach on its own, it can also be combined with fine-tuning to enhance LLM capabilities further. Fine-tuning allows models to be explicitly trained on specific datasets, honing them for certain tasks. When coupled with RAG, fine-tuned models can make informed decisions using retrieved external knowledge, yielding even more precise and task-specific results.\n",
    "\n",
    "### 6. The Advent of LLMOps:\n",
    "With the rise of large language models and their multifaceted applications, there's an emerging discipline known as LLMOps, focused on the operational aspects of deploying, monitoring, and maintaining LLM systems. RAG, in-context learning, and LLM pipelines become central topics in this space, paving the way for next-generation machine learning operations tailored to the specific needs of LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f409d0b8-ca60-45fd-b183-105217d3a539",
   "metadata": {},
   "source": [
    "Install dependencies. Only run the first line if you are running this on the Intel Developer Cloud's Jupyter Environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91658eca-f4c4-4a66-aae6-6dda4e55273c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!source /opt/intel/oneapi/setvars.sh #comment out if not running on Intel Developer Cloud Jupyter\n",
    "!pip install langchain==0.0.330\n",
    "!pip install pygpt4all==1.1.0\n",
    "!pip install gpt4all==1.0.12\n",
    "!pip install transformers==4.30.2\n",
    "!pip install datasets==2.14.6\n",
    "!pip install tiktoken==0.4.0\n",
    "!pip install chromadb==0.4.15\n",
    "!pip install sentence_transformers==2.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4690537c-65ff-4c76-a470-f5e7ce1fba1c",
   "metadata": {},
   "source": [
    "Set the model initialization parameters: \n",
    "\n",
    "**model_path = './ggml-model-gpt4all-falcon-q4_0.bin'**: This line specifies the file path for a pre-trained model, possibly a GPT-4 variant named 'gpt4all-falcon-q4_0', which will be loaded for further tasks.\n",
    "\n",
    "**n_threads = 32**: Sets the number of threads to be used, which might influence parallel processing or inference speed, especially relevant for multi-core systems.\n",
    "\n",
    "**max_tokens = 50**: Limits the number of tokens (words or subwords) to a maximum of 50 for the input or output sequences, ensuring that the data fed into or produced by the model does not exceed this length.\n",
    "\n",
    "**repeat_penalty = 1.20**: This parameter possibly penalizes repetitive content in the model's output. A value greater than 1.0 means the model is discouraged from generating repeated sequences.\n",
    "\n",
    "**n_batch = 32**: Specifies the batch size for processing data, meaning that the model will process 32 samples at a time. This can help optimize processing speed and memory usage.\n",
    "\n",
    "**top_k = 1**: This could be related to the \"top-k\" sampling strategy during the model's generation. When generating text, the model will consider only the top k most probable next tokens. In this case, only the most probable next token is considered.\n",
    "\n",
    "In summary, these lines of code are configuration settings for loading a specific pre-trained model and setting various parameters for its operation, including how data is processed and how text is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15180fe3-b73b-4632-a135-5c75e35af0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './ggml-model-gpt4all-falcon-q4_0.bin'\n",
    "n_threads =32\n",
    "max_tokens = 50\n",
    "repeat_penalty = 1.20\n",
    "n_batch = 32\n",
    "top_k = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d18bcb-2fa0-4a85-ab6d-8bdfc9e2c7db",
   "metadata": {},
   "source": [
    "The code below imports necessary libraries like os, requests, tqdm, and the GPT4All module from langchain.llms. It's designed to download a gpt4all-falcon-q4_0 language model from a specified URL on the Hugging Face platform. Using the requests library, the model is streamed and downloaded in manageable chunks. The progress of the download is visually represented using a progress bar from the tqdm library. The downloaded model is then saved to a local file, as defined by the model_path variable, in binary mode. Users are cautioned that due to the model's size, the download might take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771c018a-7609-461f-9d87-dd257425be42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "from tqdm import tqdm\n",
    "from langchain.llms import GPT4All\n",
    "\n",
    "# download the commercial gpt4all-j model\n",
    "url = \"https://huggingface.co/nomic-ai/gpt4all-falcon-ggml/resolve/main/ggml-model-gpt4all-falcon-q4_0.bin\"\n",
    "# send a GET request to the URL to download the file. Stream since it's large\n",
    "response = requests.get(url, stream=True)\n",
    "# open the file in binary mode and write the contents of the response to it in chunks\n",
    "# This is a large file, so be prepared to wait.\n",
    "with open(model_path, 'wb') as f:\n",
    "    for chunk in tqdm(response.iter_content(chunk_size=10000)):\n",
    "        if chunk:\n",
    "            f.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c323eb8-2353-442d-9248-5a5601d1273e",
   "metadata": {},
   "source": [
    "The code below initializes and loads the model into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11d6f55-e5a3-4115-ade2-f606a6517c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# Callbacks support token-wise streaming\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "# Verbose is required to pass to the callback manager\n",
    "llm = GPT4All(model=model_path, callbacks=callbacks, verbose=True,\n",
    "              n_threads=n_threads, n_predict=max_tokens, repeat_penalty=repeat_penalty,\n",
    "              n_batch=n_batch, top_k=top_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb7e1fe-753c-4a18-b7cb-55611bea0e66",
   "metadata": {},
   "source": [
    "The code below imports the langchain library functions and downloads a sample dataset of dialogues from Hugging Face using the datasets library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4310d352-94bb-495c-95b8-706c6769a4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "data = './dialogues.txt'\n",
    "\n",
    "# Download the customer service robot support dialogue from hugging face\n",
    "dataset = load_dataset(\"FunDialogues/customer-service-robot-support\")\n",
    "# Convert the dataset to a pandas dataframe\n",
    "dialogues = dataset['train']\n",
    "df = pd.DataFrame(dialogues, columns=['id', 'description', 'dialogue'])\n",
    "# Print the first 5 rows of the dataframe\n",
    "df.head()\n",
    "# only keep the dialogue column\n",
    "dialog_df = df['dialogue']\n",
    "\n",
    "# save the data to txt file\n",
    "dialog_df.to_csv(data, sep=' ', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d46749-90ae-4d72-b7b9-88f5229d0bfe",
   "metadata": {},
   "source": [
    "Below, we establish the chunk size and chunk overlap. \n",
    "\n",
    "If you set chunk_size to 500 and overlap to 25 using the RecursiveCharacterTextSplitter, when processing a text, the splitter would attempt to divide it into chunks of no more than 500 characters each. However, each chunk would overlap with its neighboring chunks by 25 characters. This means, for example, that characters 476-500 of the first chunk would be the same as characters 1-25 of the second chunk. Such overlap ensures that context is not lost between chunks, especially useful if the boundaries might cut off important contextual information. This setup aids in preserving continuity and context, particularly crucial when analyzing or processing the resulting chunks separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c127df-b667-423d-b9c5-918e19784b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 500\n",
    "overlap = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f1c902-9795-4f6e-8cf0-6acca5c577a9",
   "metadata": {},
   "source": [
    "The below code initializes a TextLoader to preprocess a given dataset, then employs a RecursiveCharacterTextSplitter to segment this data into overlapping chunks. Using the VectorstoreIndexCreator, it converts each text chunk into a numerical vector with the help of the HuggingFaceEmbeddings. These vectors are then stored in an index, presumably for efficient searching or similarity checks, within a chromadb vector DB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394cfe59-472a-4240-9d91-fc5e96e226ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(data)\n",
    "# Text Splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
    "# Embed the document and store into chroma DB\n",
    "index = VectorstoreIndexCreator(embedding= HuggingFaceEmbeddings(), text_splitter=text_splitter).from_loaders([loader])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e592882e-3afb-4a83-a3e6-6f0f51176507",
   "metadata": {},
   "source": [
    "Below we provide user input to the RAG based LLM Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ee4b04-a865-43d6-bae1-4c07b4c73844",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = 'my robot is not turning on, can you help me please?'\n",
    "context_verbosity = False\n",
    "top_k = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feab8624-896a-45e5-8e19-6ef24c0288d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# perform a similarity search and retrieve the context from our documents\n",
    "results = index.vectorstore.similarity_search(user_input, k=top_k)\n",
    "# join all context information (top 4) into one string \n",
    "context = \"\\n\".join([document.page_content for document in results])\n",
    "if context_verbosity:\n",
    "    print(f\"Retrieving information related to your question...\")\n",
    "    print(f\"Found this content which is most similar to your question: {context}\")\n",
    "template = \"\"\"\n",
    "Please use the following robotic technical support related questions to answer questions. \n",
    "Context: {context}\n",
    "---\n",
    "This is the user's question: {question}\n",
    "Answer: This is what our robot arm technical specialist suggest.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"]).partial(context=context)\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "        \n",
    "print(\"Processing the information with gpt4all...\\n\")\n",
    "start_time = time.time()\n",
    "response = llm_chain.run(user_input)\n",
    "elapsed_time_milliseconds  = (time.time() - start_time) * 1000\n",
    "\n",
    "tokens = len(response.split())\n",
    "time_per_token_milliseconds = elapsed_time_milliseconds  / tokens if tokens != 0 else 0\n",
    "\n",
    "processed_reponse = response + f\" --> {time_per_token_milliseconds:.4f} milliseconds/token AND Time taken for response: {elapsed_time_milliseconds:.2f} milliseconds\"\n",
    "\n",
    "processed_reponse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4178e9a2-fec6-4609-98d3-25e7c9329665",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this Jupyter notebook, we delve into the sophisticated realm of Retrieval Augmented Generation (RAG) and its integration with Large Language Models (LLMs) using the LangChain platform. We explore the essence of RAG, in-context learning, and the operational intricacies of LLMs, elucidating how they interplay in today's AI landscape. Harnessing LangChain's robust toolset—ranging from diverse document loaders and transformers, a plethora of text embedding models, to efficient vector stores—we craft a RAG-based chatbot specifically designed to answer intricate queries about robotic maintenance. Through this hands-on approach, learners gain a holistic understanding of the potential of RAG and the versatility of the LangChain framework.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cde56e7-0776-48d2-b114-00cc542bd4ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
